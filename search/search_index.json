{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ViVa-SAFELAND: A Visual Validation Safe Landing Simulation Platform","text":"<p>ViVa-SAFELAND is an open-source simulation platform for testing and evaluating vision-based navigation strategies for unmanned aerial vehicles, with a special focus on autonomous landing in compliance with safety regulations.</p> ViVa-SAFELAND: A Visual Validation Safe Landing Tool <p>This documentation contains the official implementation for the paper \"ViVa-SAFELAND: An Open-Source Simulation Platform for Safe Validation of Vision-based Navigation in Aerial Vehicles\". It provides a safe, simple, and fair comparison baseline to evaluate and compare different visual navigation solutions under the same conditions.</p> Example of ViVa-SAFELAND operation"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Real-World Scenarios: Utilize a collection of high-definition aerial videos from unstructured urban environments, including dynamic obstacles like cars and people.</li> <li>Emulated Aerial Vehicle (EAV): Navigate within video scenarios using a virtual moving camera that responds to high-level commands.</li> <li>Standardized Evaluation: Provides a safe and fair baseline for comparing different visual navigation solutions under identical, repeatable conditions.</li> <li>Development &amp; Data Generation: Facilitates the rapid development of autonomous landing strategies and the creation of custom image datasets for training machine learning models.</li> <li>Safety-Focused: Enables rigorous testing and debugging of navigation logic in a simulated environment, eliminating risks to hardware and ensuring compliance with safety regulations.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed usage instructions, examples, and API documentation, please refer to the ViVa-SAFELAND Documentation.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use ViVa-SAFELAND in your research, please cite us.</p>"},{"location":"about/","title":"About","text":"<p>This work was supported by the Office of Naval Research Global ONRG, Award No. <code>N62909-24-1-2001</code>.</p>"},{"location":"about/#license","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2024 ViVa-SAFELAND\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"api/","title":"Classes","text":""},{"location":"api/#viva.env.DroneEnv","title":"<code>DroneEnv(render_mode=None, video='', render_fps=30, terminated_step=3600, fixed=False, rel_alt_value=None, show_fps_flag=False)</code>","text":"<p>A simulated environment for validating vision-based drone navigation.</p> <p>This class provides a Gymnasium-like environment for a drone navigating in a simulated world. It handles rendering, physics, and state management.</p> <p>Parameters:</p> Name Type Description Default <code>render_mode</code> <code>Optional[str]</code> <p>The rendering mode ('human' or 'rgb_array').</p> <code>None</code> <code>video</code> <code>str</code> <p>The path to the background video.</p> <code>''</code> <code>render_fps</code> <code>int</code> <p>The frames per second for rendering.</p> <code>30</code> <code>terminated_step</code> <code>int</code> <p>The step at which the environment is considered terminated (at 30 FPS 3600 are 2 minutes).</p> <code>3600</code> <code>fixed</code> <code>bool</code> <p>Whether the background is a fixed image or a video.</p> <code>False</code> <code>rel_alt_value</code> <code>Optional[float]</code> <p>Initial relative altitude of the drone in meters. If not provided, it will be extracted from the video metadata or SRT file.</p> <code>None</code> <code>show_fps_flag</code> <code>bool</code> <p>Whether to display the FPS on terminal.</p> <code>False</code>"},{"location":"api/#viva.env.DroneEnv.render","title":"<code>render()</code>","text":"<p>Renders the environment.</p> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The rendered frame, if render_mode is 'rgb_array'.</p> Example <p>You can see an example of the rendering method on usage documentation.</p>"},{"location":"api/#viva.env.DroneEnv.reset","title":"<code>reset(x=None, y=None, z=None, psi_deg_init=None)</code>","text":"<p>Resets the environment to an initial state.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[float]</code> <p>Initial x position of the drone in meters. If None, a random value is chosen.</p> <code>None</code> <code>y</code> <code>Optional[float]</code> <p>Initial y position of the drone in meters. If None, a random value is chosen.</p> <code>None</code> <code>z</code> <code>Optional[float]</code> <p>Initial z position of the drone in meters. If None, a random value is chosen between 20 and (min(60, self.height - 1)).</p> <code>None</code> <code>psi_deg_init</code> <code>Optional[float]</code> <p>Initial yaw in degrees. If None, a random value is chosen between 0 and 360.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>observation</code> <code>ndarray</code> <p>The current drone camera view (RGB image).</p> <code>info</code> <code>Dict[str, Any]</code> <p>A dictionary containing auxiliary information, such as 'points' (coordinates for rendering), 'drone_state' (position and velocity), and 'actions' (the actions taken).</p>"},{"location":"api/#viva.env.DroneEnv.step","title":"<code>step(actions)</code>","text":"<p>Executes one time step in the environment.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>ndarray</code> <p>The actions to take in the environment:</p> <ul> <li>actions[0]: phi (roll) in degrees.</li> <li>actions[1]: theta (pitch) in degrees.</li> <li>actions[2]: psi velocity (yaw rate) in degrees per second.</li> <li>actions[3]: fk (Thrust) in newtons.</li> </ul> required <p>Returns:</p> Name Type Description <code>observation</code> <code>ndarray</code> <p>The current drone camera view (RGB image).</p> <code>terminated</code> <code>bool</code> <p>Whether the episode has terminated. This occurs when:</p> <ul> <li>The simulation reaches 5000 steps (approximately 166 seconds at 30 FPS).</li> <li>The virtual drone moves outside the simulation boundaries.</li> </ul> <code>info</code> <code>Dict[str, Any]</code> <p>A dictionary containing auxiliary information:</p> <ul> <li>'points' (coordinates for rendering).</li> <li>'drone_state' (position and velocity).</li> <li>'actions' (the actions taken, including the current yaw angle).</li> </ul>"},{"location":"api/#viva.modules.hmi.HMI","title":"<code>HMI(dead_zone=0.1)</code>","text":"<p>Human-Machine Interface for controlling drone environments.</p> <p>This class handles both joystick controller and keyboard inputs to generate control actions for a simulation. It provides universal controller support with consistent mappings across different controller types.</p> <p>Parameters:</p> Name Type Description Default <code>dead_zone</code> <code>float</code> <p>The dead zone threshold for joystick analog inputs (0.0 to 1.0).</p> <code>0.1</code>"},{"location":"api/#viva.modules.hmi.HMI.__call__","title":"<code>__call__()</code>","text":"<p>Processes input and returns the current control state.</p> <p>This method makes the HMI instance callable and should be used in the main control loop. It processes all pending events and updates control variables.</p> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool, bool]</code> <p>A tuple containing:</p> <ul> <li>actions (np.ndarray): Control values [theta, phi, psi_velocity, fk].<ul> <li>actions[0]: theta (pitch) in degrees.</li> <li>actions[1]: phi (roll) in degrees.</li> <li>actions[2]: psi_velocity (yaw rate) in degrees per second.</li> <li>actions[3]: fk (Thrust) in newtons.</li> </ul> </li> <li>reset_commanded (bool): True if a reset was requested.</li> <li>exit_requested (bool): True if an exit was requested.</li> </ul>"},{"location":"api/#viva.modules.hmi.HMI.quit","title":"<code>quit()</code>","text":"<p>Cleans up and shuts down the HMI system.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides instructions to install ViVa-SAFELAND.</p> <p>System Requirements</p> <p>The code automatically creates a Python 3.12 virtual environment. However, the code has been tested and works with Python versions starting from 3.8.</p>"},{"location":"installation/#1-setting-uv-the-python-project-manager","title":"1. Setting <code>UV</code>, the Python project manager","text":"<p>To facilitate the creation of virtual environments and manage Python packages and their dependencies we use a state of the art framework uv, its installation is straightforward and can be done via the following command:</p> macOS/LinuxWindows <p>Using <code>curl</code> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> Using <code>wget</code> <pre><code>wget -qO- https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>Use <code>irm</code> to download the script and execute it with <code>iex</code>: <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p>"},{"location":"installation/#2-install-viva-safeland","title":"2. Install ViVa-SAFELAND","text":"<p>Choose one of the following installation methods:</p> From PyPIFrom Source <p>Recommended for most users</p> <p>Install the latest stable release from the Python Package Index (PyPI).</p> <pre><code>uv venv --python 3.12\nuv pip install viva-safeland --upgrade\n</code></pre> <p>Recommended for developers</p> <p>Install the latest development version directly from the GitHub repository.</p> <pre><code>git clone https://github.com/viva-safeland/viva_safeland.git\ncd viva_safeland\nuv sync\n</code></pre>"},{"location":"installation/#3-high-definition-aerial-videos","title":"3. High-definition Aerial Videos","text":"<p>To use ViVa-SAFELAND, you need to have high-definition aerial videos. You can use your own videos or download the provided dataset</p>"},{"location":"installation/#using-your-own-videos","title":"Using your own videos","text":"<p>To use your own videos, they should follow these specifications:</p> <ul> <li>Format: <code>.MP4</code></li> <li>Resolution: 4K (3840x2160 pixels)</li> <li>Frame Rate: 30 FPS by default, if your video has a different frame rate, you can specify it (see usage for more information).</li> <li> <p>Metadata: You need to provide the initial altitude of the drone, an <code>.SRT</code> file with the same name as the video file is regularly created by the drone. The <code>.SRT</code> file should contain the text <code>rel_alt: &lt;initial_altitude&gt;</code> with the initial altitude of the drone in meters. The file should be in the same directory as the video file.</p> <p>Here is an example of a directory containing a video file and its corresponding metadata file:</p> <pre><code>/media/user/HDD/Videos_Dron/DJI_20240910181532_0005_D.MP4\n/media/user/HDD/Videos_Dron/DJI_20240910181532_0005_D.SRT\n</code></pre> <p>The <code>.SRT</code> generated automatically by the drone should look like this:</p> <pre><code>1\n00:00:00,000 --&gt; 00:00:00,033\n&lt;font size=\"28\"&gt;FrameCnt: 1, DiffTime: 33ms\n2024-09-10 18:15:32.300\n[iso: 110] [shutter: 1/320.0] [fnum: 1.7] [ev: 0] [color_md: default] [focal_len: 24.00] [latitude: 22.764504] [longitude: -102.550488] [rel_alt: 79.900 abs_alt: 2513.263] [ct: 5729] &lt;/font&gt;\n</code></pre> <p>If you don't have the generated <code>.SRT</code> file you can specify the initial altitude of the drone (see usage for more information).</p> </li> </ul>"},{"location":"installation/#download-the-viva-safeland-dataset","title":"Download the ViVa-SAFELAND Dataset","text":"<p>Alternatively, you can download the provided ViVa-SAFELAND Dataset which contains a collection of high-definition aerial videos from unstructured urban environments, including dynamic obstacles like cars, people and other moving objects.  This dataset follows the specifications mentioned above and is ready to use with ViVa-SAFELAND.</p> <ol> <li>Create a directory to store the dataset, (it is not necessary to be in the same directory of the project, due the size you maybe want to store it in a different location like an external hard drive).</li> <li>Download the desired videos with its metadata from Zenodo.</li> <li>Extract the downloaded files into the created directory.</li> <li>Follow the instructions of usage.</li> </ol>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide will walk you through a basic example of how to use ViVa-SAFELAND.</p> <p>First, make sure you have the necessary dependencies installed. Then, you can run the simulation directly from your terminal</p>"},{"location":"usage/#quick-start","title":"Quick Start","text":""},{"location":"usage/#graphical-user-interface-gui","title":"Graphical User Interface (GUI)","text":"<p>You can start the simulator with this command line:</p> <pre><code>uv run viva\n</code></pre> <p></p> <p>This command will start a GUI where you can configure the simulation parameters:</p> <ul> <li>Video Drone Configuration:<ul> <li>Video Directory: This is the directory where your video files are located.</li> <li>Video File: This is the specific video file you want to use for the simulation.</li> </ul> </li> <li>Environment Configuration:<ul> <li>FPS: This is the frames per second setting for the simulation (default is 30).</li> <li>Show FPS: If set, the FPS will be displayed on the terminal.</li> <li>Fixed: If set, the background will be only the first frame of the video sequence.</li> <li>Use Auto Altitude: If set, the initial altitude will be automatically determined from the video metadata (*.SRT file), default is <code>True</code>.</li> <li>Altitude: If you don't have the <code>.SRT</code> file or you want to specify a different altitude, you can use this option, this option is enabled when <code>Use Auto Altitude</code> is unchecked.</li> </ul> </li> <li>Simulation Control:<ul> <li>Start: Begin the simulation with the configured parameters.</li> <li>Stop: Stop the simulation.</li> </ul> </li> </ul>"},{"location":"usage/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>Alternatively, you can run the simulator specifying the options directly in terminal, this method does not activate the GUI.</p> <pre><code>uv run viva \"/path/to/your/drone_video.mp4\"\n</code></pre>"},{"location":"usage/#options","title":"Options","text":"<p>You can customize the simulation as in the GUI with the options:</p> <ul> <li><code>--help</code>: Show info about the usage of viva.</li> <li><code>--render-fps &lt;fps&gt;</code>: Set the frames per second for rendering (default is 30).</li> <li><code>--fixed</code>: If set, the background will be only the first frame of the video sequence.</li> <li><code>--rel-alt-value &lt;initial_altitude&gt;</code>: Set the initial relative altitude of the drone in meters. If not provided, it will be extracted from the video metadata or SRT file.</li> <li><code>--show-fps-flag</code>: If set, the FPS will be displayed on the terminal.</li> </ul> <p>Example with options:</p> <pre><code>viva \"/path/to/your/drone_video.mp4\" --render-fps 60 --fixed --rel-alt-value 80 --show-fps-flag\n</code></pre>"},{"location":"usage/#controls","title":"Controls","text":"<p>You can start to move the drone with the <code>keyboard</code> or a <code>joystick</code> if you connect one. The default controls are:</p> Keyboard ControlsJoystick Controls <p></p> <p>Thrust and orientation</p> <ul> <li>W/S: Increase/Decrease Thrust</li> <li>A/D: Yaw anticlockwise/clockwise</li> </ul> <p>Movement</p> <ul> <li>Up/Down Arrow: Move Forward/Backward</li> <li>Left/Right Arrow: Move Left/Right</li> </ul> <p>Special Actions</p> <ul> <li>R: Reset drone state</li> <li>Esc: Exit from the simulation</li> </ul> <p></p> <p>Thrust and orientation</p> <ul> <li>Left Stick Up/Down: Increase/Decrease Thrust</li> <li>Left Stick Left/Right: Yaw anticlockwise/clockwise</li> </ul> <p>Movement</p> <ul> <li>Right Stick Up/Down: Move Forward/Backward</li> <li>Right Stick Left/Right: Move Left/Right</li> </ul> <p>Special Actions</p> <ul> <li>BACK button: Reset drone state</li> <li>B button: Exit from the simulation</li> </ul> <p>Note</p> <p>The controls of the joystick are universally mapped, so the positions of the sticks and buttons are the same for all joysticks. </p>"},{"location":"usage/#application-usage","title":"Application Usage","text":"<p>If you want to use <code>viva</code> to develop your own application, you can use the <code>DroneEnv</code> class from the <code>viva</code> package. Below is a minimal example of how to use it:</p> render_mode=humanrender_mode=rgb_array Minimal usage example<pre><code>from viva import DroneEnv\n\n# Create an instance of the simulator environment\nenv = DroneEnv(\n    render_mode=\"human\", \n    video=\"&lt;path_to_your_video_file&gt;.MP4\",\n)\n\n# Initialize the environment with a random drone pose\nobs, info = env.reset()\nterminated = False\n\n# Main loop to run the simulation\nwhile not terminated:\n    action = [0.0, 0.0, 0.0, 0.0]               # Example: no action\n    obs, terminated, info = env.step(action)\n</code></pre> Minimal usage example<pre><code>from viva import DroneEnv\nimport cv2\n\n# Create an instance of the simulator environment\nenv = DroneEnv(\n    render_mode=\"rgb_array\", \n    video=\"&lt;path_to_your_video_file&gt;.MP4\",\n)\n\n# Initialize the environment with a random drone pose\nobs, info = env.reset()\nterminated = False\n\n# Main loop to run the simulation\nwhile not terminated:\n    action = [0.0, 0.0, 0.0, 0.0]               # Example: no action\n    obs, terminated, info = env.step(action)\n    img = env.render()                          # Get the RGB array\n    cv2.imshow(\"environment\", img)              # Display the image\n    if cv2.waitKey(1) == 27:                    # Exit on 'ESC' key\n        break\ncv2.destroyAllWindows()\n</code></pre> <p>Useful information</p> <p><code>obs</code> is a RGB array representing the current drone view and one of the most important outputs of the environment, you can treat it as an image and apply your computer vision algorithms on it.</p> <p>The full documentation of the <code>DroneEnv</code> class can be found in the API reference.</p>"},{"location":"usage/#example-with-controller","title":"Example with Controller","text":"<p>To add a controller to the drone you can import the HMI class from the <code>viva</code> package and use it to control the drone with a keyboard or joystick. Below is an example of how to integrate the controller:</p> Adding a Controller<pre><code>    from viva import DroneEnv\n    from viva import HMI\n\n    # Create an instance of the simulator environment\n    env = DroneEnv(\n        render_mode=\"human\", \n        video=\"&lt;path_to_your_video_file&gt;.MP4\",\n    )\n\n    # Create an instance of the HMI controller\n    hmi = HMI()\n\n    # Initialize the environment with a random drone pose\n    obs, info = env.reset()\n    terminated = False\n\n    # Main loop to run the simulation\n    while not terminated:\n        action, reset, terminated_command = hmi()\n        obs, terminated, info = env.step(action)\n        terminated = terminated or terminated_command\n        if reset:\n            obs, info = env.reset()\n    hmi.quit()\n</code></pre> <p>For full documentation of the <code>HMI</code> class, see the API reference.</p>"}]}